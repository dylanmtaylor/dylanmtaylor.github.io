<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.1">Jekyll</generator><link href="https://dylanmtaylor.com/pages/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://dylanmtaylor.com/pages/blog/" rel="alternate" type="text/html" /><updated>2018-05-04T19:40:32+00:00</updated><id>https://dylanmtaylor.com/pages/blog/</id><entry><title type="html">Using BackBlaze B2 Cloud Storage for Encrypted Offsite Backups</title><link href="https://dylanmtaylor.com/pages/blog/2018/05/04/using_backblaze_b2_cloud_storage_for_encrypted_backups/" rel="alternate" type="text/html" title="Using BackBlaze B2 Cloud Storage for Encrypted Offsite Backups" /><published>2018-05-04T01:04:33+00:00</published><updated>2018-05-04T01:04:33+00:00</updated><id>https://dylanmtaylor.com/pages/blog/2018/05/04/using_backblaze_b2_cloud_storage_for_encrypted_backups</id><content type="html" xml:base="https://dylanmtaylor.com/pages/blog/2018/05/04/using_backblaze_b2_cloud_storage_for_encrypted_backups/">&lt;p&gt;As of right now, one of the cheapest offsite backup solutions by far is &lt;a href=&quot;https://www.backblaze.com/b2/cloud-storage.html&quot;&gt;BackBlaze B2&lt;/a&gt;, with &lt;a href=&quot;https://aws.amazon.com/s3/&quot;&gt;Amazon S3&lt;/a&gt; trailing behind. For a while now, I’ve been using this solution with a piece of software called &lt;a href=&quot;https://restic.net/&quot;&gt;restic&lt;/a&gt; to store encrypted backups of some of my important documents offsite. By doing so, should my Western Digital 8TB external hard drive (which I currently use for all of my backups) fail, or get stolen (it’s strongly encrypted, so my data is safe), I have a solution to get the files I need the most back. While I store anything and everything on my external, I am limiting the files in the offsite backup to things that are important. It’s not economical to push things like your music/movie collection, Linux ISOs and virtual machine hard drive images to the BackBlaze cloud. Ubuntu’s repositories contain the restic binaries, but other distributions may require you to install a new repository or build from source. I could write up a guide on how to set this up, but &lt;a href=&quot;https://fedoramagazine.org/&quot;&gt;Fedora Magazine&lt;/a&gt; wrote &lt;a href=&quot;https://fedoramagazine.org/use-restic-encrypted-backups/&quot;&gt;an excellent article on how to setup this exact solution&lt;/a&gt;.  I’d be doing anyone reading this a disservice trying to write up an article that better explains the setup steps. If you’re not a huge fan of restic, &lt;a href=&quot;http://duplicity.nongnu.org/&quot;&gt;duplicity&lt;/a&gt; also supports BackBlaze B2, and is fairly easy to setup from what I hear.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Dylan Taylor&quot;, &quot;login&quot;=&gt;&quot;dylanmtaylor&quot;, &quot;email&quot;=&gt;&quot;dylan@dylanmtaylor.com&quot;}</name><email>dylan@dylanmtaylor.com</email></author><summary type="html">As of right now, one of the cheapest offsite backup solutions by far is BackBlaze B2, with Amazon S3 trailing behind. For a while now, I’ve been using this solution with a piece of software called restic to store encrypted backups of some of my important documents offsite. By doing so, should my Western Digital 8TB external hard drive (which I currently use for all of my backups) fail, or get stolen (it’s strongly encrypted, so my data is safe), I have a solution to get the files I need the most back. While I store anything and everything on my external, I am limiting the files in the offsite backup to things that are important. It’s not economical to push things like your music/movie collection, Linux ISOs and virtual machine hard drive images to the BackBlaze cloud. Ubuntu’s repositories contain the restic binaries, but other distributions may require you to install a new repository or build from source. I could write up a guide on how to set this up, but Fedora Magazine wrote an excellent article on how to setup this exact solution. I’d be doing anyone reading this a disservice trying to write up an article that better explains the setup steps. If you’re not a huge fan of restic, duplicity also supports BackBlaze B2, and is fairly easy to setup from what I hear.</summary></entry><entry><title type="html">Awesome New Share Bar Added to My Blog</title><link href="https://dylanmtaylor.com/pages/blog/2018/05/04/awesome_new_share_bar_added_to_my_blog/" rel="alternate" type="text/html" title="Awesome New Share Bar Added to My Blog" /><published>2018-05-04T00:44:21+00:00</published><updated>2018-05-04T00:44:21+00:00</updated><id>https://dylanmtaylor.com/pages/blog/2018/05/04/awesome_new_share_bar_added_to_my_blog</id><content type="html" xml:base="https://dylanmtaylor.com/pages/blog/2018/05/04/awesome_new_share_bar_added_to_my_blog/">&lt;p&gt;For the longest time, I’ve had no social media sharing buttons on this site other than a &lt;a href=&quot;https://plus.google.com/&quot;&gt;Google Plus&lt;/a&gt; button. I decided to remove the Disqus comments section from the site since nobody was using it anyways, and realized that I could take the opportunity to update the interface. I searched the web for &lt;a href=&quot;https://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt; social media share buttons and came across &lt;a href=&quot;https://mycyberuniverse.com/web/social-media-share-bar-jekyll-blog-website.html&quot;&gt;an amazing blog post explaining how to add them with some sample code&lt;/a&gt;. After following the tutorial and modifying the HTML and CSS a little bit to better fit with how I want the buttons to be displayed, I am pleased to say that you can now click the share buttons to quickly post a link to any of my content on many different platforms and even email it. I’ve got support for Facebook, Twitter, Reddit, Google Plus, Tumblr and LinkedIn enabled. One feature of this solution that I really like is that the buttons have icons on them from the &lt;a href=&quot;https://fontawesome.com/&quot;&gt;Font Awesome&lt;/a&gt; project. These are all vector social logos that should render quite nicely on any modern browser. Feel free to try it out below if you’d like. It will prompt you to edit the post before sharing it.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Dylan Taylor&quot;, &quot;login&quot;=&gt;&quot;dylanmtaylor&quot;, &quot;email&quot;=&gt;&quot;dylan@dylanmtaylor.com&quot;}</name><email>dylan@dylanmtaylor.com</email></author><summary type="html">For the longest time, I’ve had no social media sharing buttons on this site other than a Google Plus button. I decided to remove the Disqus comments section from the site since nobody was using it anyways, and realized that I could take the opportunity to update the interface. I searched the web for Jekyll social media share buttons and came across an amazing blog post explaining how to add them with some sample code. After following the tutorial and modifying the HTML and CSS a little bit to better fit with how I want the buttons to be displayed, I am pleased to say that you can now click the share buttons to quickly post a link to any of my content on many different platforms and even email it. I’ve got support for Facebook, Twitter, Reddit, Google Plus, Tumblr and LinkedIn enabled. One feature of this solution that I really like is that the buttons have icons on them from the Font Awesome project. These are all vector social logos that should render quite nicely on any modern browser. Feel free to try it out below if you’d like. It will prompt you to edit the post before sharing it.</summary></entry><entry><title type="html">Blocking Annoying Web Advertisements at the DNS Level with Pi-Hole</title><link href="https://dylanmtaylor.com/pages/blog/2018/05/03/blocking_annoying_web_advertisements_at_the_dns_level_with_pi-hole/" rel="alternate" type="text/html" title="Blocking Annoying Web Advertisements at the DNS Level with Pi-Hole" /><published>2018-05-03T22:39:19+00:00</published><updated>2018-05-03T22:39:19+00:00</updated><id>https://dylanmtaylor.com/pages/blog/2018/05/03/blocking_annoying_web_advertisements_at_the_dns_level_with_pi-hole</id><content type="html" xml:base="https://dylanmtaylor.com/pages/blog/2018/05/03/blocking_annoying_web_advertisements_at_the_dns_level_with_pi-hole/">&lt;p&gt;&lt;a href=&quot;https://pi-hole.net/&quot;&gt;&lt;img src=&quot;/images/blog/2018/05/Vortex-R.png&quot; alt=&quot;Pi-Hole&quot; width=&quot;100px&quot; height=&quot;100px&quot; style=&quot;float:left&quot; /&gt;&lt;/a&gt; If you’re like myself and many other users on the internet, you probably hate advertisements. Not only are they annoying with automatically playing videos, covering up web content and being a vector for malware transmission from unscrupulous advertisement platform, but they can drain your battern on your phone and eat up your bandwidth if you are on a fixed data plan.&lt;/p&gt;

&lt;p&gt;There are many reasons to hate advertisements but fortunately users are fighting back. One way is through browser extensions. I personally use a free and open source one called &lt;a href=&quot;https://github.com/gorhill/uBlock&quot;&gt;uBlock Origin&lt;/a&gt; on all of my devices. It is independently operated, lightweight and does not support the acceptable ads program, unlike Adblok Plus, which is partially funded by advertisers paying to get unblocked. See the conflict of interest here?&lt;/p&gt;

&lt;p&gt;While these extensions are fantastic, they’re only a start. Unfortunately, at least in mobile applications like the one from &lt;a href=&quot;https://www.wunderground.com/&quot;&gt;Weather Underground&lt;/a&gt;, which is quite a good app by the way, there is no way to block advertisements without paying a fee to the developer. The mobile version of Chrome also does not support extensions. I’ve lot count of the number of times I’ve seen fake “Your Android has been infected” pages that make noises and vibrate the phone. It’s obnoxious. Until this isn’t the case anymore, browsing the mobile web without an adblocker is a chore.&lt;/p&gt;

&lt;p&gt;So, how can we block ads across all the devices in the network, including phones without installing special software on anything? Simple: by running our own DNS server. DNS, short for Domain Name System, is a technology that handles domain lookups. When you type dylanmtaylor.com into your browser, your browser doesn’t know where to go to make a connection from the name. It contacts a DNS server to translate the humanly readable domain name into an IP address, which your browser then uses to make the connection. If we intentionally override this using our own DNS server, we can point adomain to something else, like the DNS server itself or 127.0.0.1. By doing this, the domain is effectively blocked.&lt;/p&gt;

&lt;p&gt;This is the core concept behind &lt;a href=&quot;https://pi-hole.net/&quot;&gt;Pi-Hole&lt;/a&gt;. With nothing more than a cheap &lt;a href=&quot;https://www.raspberrypi.org/&quot;&gt;Raspberry Pi&lt;/a&gt; single board computer, you can run your own DNS server. The setup is simple and described on the &lt;a href=&quot;https://pi-hole.net/&quot;&gt;Pi-Hole&lt;/a&gt; website. Simply connect your Pi to the network, setup static IP address allocation for the Pi’s MAC address in your router’s admin interfeace, install the software (probably on top of &lt;a href=&quot;https://www.raspberrypi.org/downloads/raspbian/&quot;&gt;Raspbian Lite&lt;/a&gt;), and finally reconfigure the DNS server entries of your router to point to the local Pi-Hole server. By doing this, so long as you are connected to your wireless network, you will be exposed to significantly less advertisements unless you manually configure your devices to use an alternative DNS server such as 1.1.1.1 from CloudFlare. If you enjoy Pi-Hole, I’d like to suggest &lt;a href=&quot;https://pi-hole.net/donate/&quot;&gt;donating a few dollars to them&lt;/a&gt;. The project is maintained entirely by volunteers and could use all the help they can get to fund the development.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Dylan Taylor&quot;, &quot;login&quot;=&gt;&quot;dylanmtaylor&quot;, &quot;email&quot;=&gt;&quot;dylan@dylanmtaylor.com&quot;}</name><email>dylan@dylanmtaylor.com</email></author><summary type="html">If you’re like myself and many other users on the internet, you probably hate advertisements. Not only are they annoying with automatically playing videos, covering up web content and being a vector for malware transmission from unscrupulous advertisement platform, but they can drain your battern on your phone and eat up your bandwidth if you are on a fixed data plan.</summary></entry><entry><title type="html">Celebrate World Password Day with a Password Manager</title><link href="https://dylanmtaylor.com/pages/blog/2018/05/03/celebrate_world_password_day_with_a_password_manager/" rel="alternate" type="text/html" title="Celebrate World Password Day with a Password Manager" /><published>2018-05-03T22:08:11+00:00</published><updated>2018-05-03T22:08:11+00:00</updated><id>https://dylanmtaylor.com/pages/blog/2018/05/03/celebrate_world_password_day_with_a_password_manager</id><content type="html" xml:base="https://dylanmtaylor.com/pages/blog/2018/05/03/celebrate_world_password_day_with_a_password_manager/">&lt;p&gt;Did you know that May 3rd is &lt;a href=&quot;https://www.passwordday.org/&quot;&gt;World Password Day&lt;/a&gt;? One of the things that I suggest to all my friends is that they have a different password for every single login. I personally do this with the help of a tool called a password manager. Password managers can generate a different password for everything you use and remember it for you. I will freely admit that I don’t know 99% of my passwords, and that is a good thing, since not only are they all rather long and complex (usually I will use the longest length the site allows up to 99 characters, with maximum complexity), but they are also all unique. This is important since if you use the same password for everything you are at risk - criminals are constantly breaking into password databases, and if they manager to get one of your passwords, they have them all.&lt;/p&gt;

&lt;p&gt;There are a number of really good password managers out there. I personally use one called &lt;a href=&quot;https://www.lastpass.com/&quot;&gt;LastPass&lt;/a&gt;, and pay for the subscription. The service is worth it to me since I like to have my passwords synced across multiple devices, but you can also use it for free. Unfortunately, LastPass is not an open source tool, and you have to trust that they are protecting your data adequately.&lt;/p&gt;

&lt;p&gt;If you want a free and open source password manager, I can highly recommend &lt;a href=&quot;https://www.keepassx.org/&quot;&gt;KeePassX&lt;/a&gt;. It has the added benefit of not having to worry about syncing your data to the cloud - you control your own password list entirely. A coworker of mine introduced me to this tool, and I have since tried it out. It works really well, and can even automatically type in passwords. The only downside is that you have to worry about syncing your data manually if you wish to do so, and you are responsible for keeping password backups. If you lose these, getting access to your accounts will be quite painful.&lt;/p&gt;

&lt;p&gt;In observance of world password day, I will be reviewing and changing my older passwords in LastPass, and I suggest that everyone else do the same. This improves your overall security, and removes access to your accounts in the case that someone has managed to get your old password.&lt;/p&gt;

&lt;p&gt;While I’m writing about security, it is worth mentioning that in many cases, having a password alone is not sufficient for proper online security. One thing that everyone should be taking advantage of, especially because of the ubiquity of smartphones is two-factor authentication, or 2FA. 2FA is the idea of providing two pieces of data to login to your account - one that is based on something you know, a password, and one that is based on something you have.&lt;/p&gt;

&lt;p&gt;With modern 2FA like &lt;a href=&quot;https://play.google.com/store/apps/details?id=com.google.android.apps.authenticator2&amp;amp;hl=en_US&quot;&gt;Google Authenticator&lt;/a&gt;, a pre-shared secret key is used to mathematically generate a numerical token based on the current time. This token is only valid for 30 seconds or so. When you enroll your device, the token is only sent to the device once and cannot be retrieved afterwards, so it cannot be duplicated. Because of this, you need your phone or authenticator application, such as &lt;a href=&quot;https://authy.com/&quot;&gt;Authy&lt;/a&gt; in order to get the token needed to log in. I use two-factor authentication on pretty much everything that supports it, and this provides me with an extra layer of security, as even if you can compromise the password, a malicious actor would need to have physical access to the device containing the token in order to login.&lt;/p&gt;

&lt;p&gt;Anyways, happy password day, and stay safe out there!&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Dylan Taylor&quot;, &quot;login&quot;=&gt;&quot;dylanmtaylor&quot;, &quot;email&quot;=&gt;&quot;dylan@dylanmtaylor.com&quot;}</name><email>dylan@dylanmtaylor.com</email></author><summary type="html">Did you know that May 3rd is World Password Day? One of the things that I suggest to all my friends is that they have a different password for every single login. I personally do this with the help of a tool called a password manager. Password managers can generate a different password for everything you use and remember it for you. I will freely admit that I don’t know 99% of my passwords, and that is a good thing, since not only are they all rather long and complex (usually I will use the longest length the site allows up to 99 characters, with maximum complexity), but they are also all unique. This is important since if you use the same password for everything you are at risk - criminals are constantly breaking into password databases, and if they manager to get one of your passwords, they have them all.</summary></entry><entry><title type="html">Checking What IP Addresses Connected to Your Nginx Server</title><link href="https://dylanmtaylor.com/pages/blog/2018/05/01/checking_what_ip_addresses_connected_to_your_nginx_server/" rel="alternate" type="text/html" title="Checking What IP Addresses Connected to Your Nginx Server" /><published>2018-05-01T03:12:25+00:00</published><updated>2018-05-01T03:12:25+00:00</updated><id>https://dylanmtaylor.com/pages/blog/2018/05/01/checking_what_ip_addresses_connected_to_your_nginx_server</id><content type="html" xml:base="https://dylanmtaylor.com/pages/blog/2018/05/01/checking_what_ip_addresses_connected_to_your_nginx_server/">&lt;p&gt;My server is behind a &lt;a href=&quot;https://www.cloudflare.com&quot;&gt;CloudFlare&lt;/a&gt; caching proxy, so it usually doesn’t get hit with requests directly, but I was noticing thousands of unique IP addresses looking for files such as &lt;code&gt;/xampp/phpmyadmin/index.php&lt;/code&gt; and &lt;code&gt;/db/index.php&lt;/code&gt;. These are probably hackers using automated vulnerability scanners against anything that has an open HTTP connection on the internet. I got curious where these requests were coming from, so I decided to dig through the logs.&lt;/p&gt;

&lt;p&gt;The first thing I did was connect to my server using the hostname I have in my &lt;code&gt;/etc/hosts&lt;/code&gt; file for convenience through the GNOME file browser. I entered &lt;code&gt;sftp://root@dylanmtaylor/var/log/nginx&lt;/code&gt; as the connection string. I then copied all the access log files to my local machine into a temporary NGINX folder.&lt;/p&gt;

&lt;p&gt;From there, I ran this commmand:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;for file in $(ls *.gz); do gzip -d $file; done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This decompressed all of the gzipped log files by taking the output of &lt;code&gt;ls *.gz&lt;/code&gt; and iterating through the files to run &lt;code&gt;gzip -d&lt;/code&gt; on each one. For usage information on &lt;code&gt;gzip&lt;/code&gt;, you can run &lt;code&gt;gzip -h&lt;/code&gt;
In order to combine the log files into one for easy parsing, we can use &lt;code&gt;cat&lt;/code&gt;, which is short for concatenate. &lt;code&gt;cat access*.log*&lt;/code&gt; works effectively for this.&lt;/p&gt;

&lt;p&gt;Using UNIX input redirection, we take the output of the &lt;code&gt;cat access*.log*&lt;/code&gt; command and redirect it to the standard input of the awk process. One really useful trick of awk is to print a specific argument to it, like &lt;code&gt;'{print $1}'&lt;/code&gt; which will give you the first column of output. You can change the &lt;code&gt;$1&lt;/code&gt; to the column number of the input desired for automated parsing. I then piped that to &lt;code&gt;uniq -c&lt;/code&gt;. uniq is a utility that will find unique entries. The &lt;code&gt;-c&lt;/code&gt; argument simply shows the count – how many times each occurence showed up. Finally, because these are in no particular order, I yet again piped the output to &lt;code&gt;sort -nr&lt;/code&gt;. The &lt;code&gt;-n&lt;/code&gt; flag simply tells sort to compare based on numerical value. By default, these values are ascending (112 vs 64 would have 64 first). Because I want them in descending order, I added the &lt;code&gt;-r&lt;/code&gt; flag, which puts the ones that show up the most at the top of the list. Finally, I use &lt;code&gt;tee&lt;/code&gt; to store the results as &lt;code&gt;ip-addresses&lt;/code&gt;. The whole command looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;awk '{print $1}' &amp;lt;(cat access*.log*) | sort | uniq | sort -nr | tee ip-addresses
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I then tried to use &lt;a href=&quot;https://www.infobyip.com/ipbulklookup.php&quot;&gt;a bulk IP address lookup tool&lt;/a&gt; to check where these requests were coming from. Unfortunately, they have a limit of 100 IP addresses and they want the list to be space/new line delimited. Fortunately, there is a way to find the worst offenders. The list is already sorted descending, so I can take advantage of the &lt;code&gt;head&lt;/code&gt; tool with the &lt;code&gt;-n 100&lt;/code&gt; argument, which just takes the first 100 lines of the file or standard input and redirects it to standard output.&lt;/p&gt;

&lt;p&gt;Because I saved my output as &lt;code&gt;ip-addresses&lt;/code&gt;, I was able to simply cat this file to the necessary utilities with &lt;code&gt;cat ip-addresses | head -n 100 | awk '{print $2}'&lt;/code&gt;, but UNIX pipes are pretty flexible. The whole thing can be done in one line like so:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;awk '{print $1}' &amp;lt;(cat access*.log*) | sort | uniq | sort -nr | head -n 100 | awk '{print $2}'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Naturally, I found my own IP address and several IP addresses belonging to CloudFlare, which is expected, but I also found hits from assorted IP addresses all over the world, excluding the CloudFlare mirrors (which are in several countries), from the United States, Canada, Indonesia, China, Hong Kong, Seychelles and Russia. From this data, it is hard to easily tell which ones are malicious, and which stumbled upon the IP address, but it is fascinating to see how geographically distributed the requests to a site as small as mine are.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Dylan Taylor&quot;, &quot;login&quot;=&gt;&quot;dylanmtaylor&quot;, &quot;email&quot;=&gt;&quot;dylan@dylanmtaylor.com&quot;}</name><email>dylan@dylanmtaylor.com</email></author><summary type="html">My server is behind a CloudFlare caching proxy, so it usually doesn’t get hit with requests directly, but I was noticing thousands of unique IP addresses looking for files such as /xampp/phpmyadmin/index.php and /db/index.php. These are probably hackers using automated vulnerability scanners against anything that has an open HTTP connection on the internet. I got curious where these requests were coming from, so I decided to dig through the logs.</summary></entry><entry><title type="html">Fixing SSH Key Permissions</title><link href="https://dylanmtaylor.com/pages/blog/2018/04/28/fixing_ssh_key_permissions/" rel="alternate" type="text/html" title="Fixing SSH Key Permissions" /><published>2018-04-28T17:56:54+00:00</published><updated>2018-04-28T17:56:54+00:00</updated><id>https://dylanmtaylor.com/pages/blog/2018/04/28/fixing_ssh_key_permissions</id><content type="html" xml:base="https://dylanmtaylor.com/pages/blog/2018/04/28/fixing_ssh_key_permissions/">&lt;p&gt;If you’ve ever copied your SSH key from one sytem to another, you might see something along the lines of &lt;code&gt;permissions are too open&lt;/code&gt; or &lt;code&gt;sign_and_send_pubkey: signing failed: agent refused operation&lt;/code&gt; when trying to use SSH to connect from one server to another. Chances are the ownership or permissions of the &lt;code&gt;~/.ssh&lt;/code&gt; directory and the files inside of it are not correct. This can easily be fixed. As the user you’re trying to SSH to a server, run these commands:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo chown -R $USER:$USER ~/.ssh
sudo chmod 0600 ~/.ssh/*
sudo chmod 0700 ~/.ssh/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once you’ve tried that, you should be able to SSH to your server again. I’ve hit this issue a number of times, so I hope this post helps someone out.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Dylan Taylor&quot;, &quot;login&quot;=&gt;&quot;dylanmtaylor&quot;, &quot;email&quot;=&gt;&quot;dylan@dylanmtaylor.com&quot;}</name><email>dylan@dylanmtaylor.com</email></author><summary type="html">If you’ve ever copied your SSH key from one sytem to another, you might see something along the lines of permissions are too open or sign_and_send_pubkey: signing failed: agent refused operation when trying to use SSH to connect from one server to another. Chances are the ownership or permissions of the ~/.ssh directory and the files inside of it are not correct. This can easily be fixed. As the user you’re trying to SSH to a server, run these commands:</summary></entry><entry><title type="html">How My Server is Deployed and Configured Using Ansible</title><link href="https://dylanmtaylor.com/pages/blog/2018/04/27/how-my-server-is-deployed-and-configured-using-ansible/" rel="alternate" type="text/html" title="How My Server is Deployed and Configured Using Ansible" /><published>2018-04-27T23:01:59+00:00</published><updated>2018-04-27T23:01:59+00:00</updated><id>https://dylanmtaylor.com/pages/blog/2018/04/27/how-my-server-is-deployed-and-configured-using-ansible</id><content type="html" xml:base="https://dylanmtaylor.com/pages/blog/2018/04/27/how-my-server-is-deployed-and-configured-using-ansible/">&lt;p&gt;&lt;a href=&quot;https://www.ansible.com/&quot;&gt;&lt;img src=&quot;/images/blog/2018/04/ansible_logo_round.png&quot; alt=&quot;Ansible&quot; width=&quot;120px&quot; height=&quot;120px&quot; style=&quot;float:left; padding: 5px&quot; /&gt;&lt;/a&gt; Traditionally, deploying infrastructure was a very time and labor intensive process. Manually going in and configuring services, installing packages, applying system updates, copying over data, etc. could easily take an entire day or more of man hours. In order to solve this problem, several tools, such as &lt;a href=&quot;https://www.ansible.com/&quot;&gt;Ansible&lt;/a&gt;, &lt;a href=&quot;https://www.chef.io/chef/&quot;&gt;Chef&lt;/a&gt;, &lt;a href=&quot;https://puppet.com/&quot;&gt;Puppet&lt;/a&gt;, and &lt;a href=&quot;https://saltstack.com/&quot;&gt;SaltStack&lt;/a&gt; were created in order to make the lives of system administrators and infrastructure architects everywhere easier. I have adopted Ansible in order to configure my web server. Honestly, the way that my server is configured, there is nothing on it that I cannot afford to lose. With very little effort, I can re-run the Ansible “playbook”, and an identical serer will be created within a couple hours. This is a concept called immutable infrastructure. If there is something wrong with the server, it makes more sense to simply replace it rather than reconfigure or repair it.&lt;/p&gt;

&lt;p&gt;Think of an Ansible playbook as a “blueprint” for the server. It contains a bunch of rules that are used to validate the state of things, and take corrective action if a deviation is detected.  By default, this means that properly written Ansible playbooks are idempotent. For instance, I can write a rule that says a file contains a line, or a package is the latest version. If the criteria is already met, no action will be taken, but if not the correction will be made. Playbooks are broken down into &lt;code&gt;roles&lt;/code&gt;. Roles are a set of tasks that Ansible evaluates. For instance, in order to configure sshd and firewalld, I am using a security role. For my webserver, I have an nginx role. These roles are called by the &lt;code&gt;playbook.yml&lt;/code&gt; file against all the hosts in the defined group in the &lt;code&gt;hosts&lt;/code&gt; file local to &lt;a href=&quot;https://github.com/dylanmtaylor/dylanmtaylor-ansible&quot;&gt;my Ansible playbook repository&lt;/a&gt;. This playbook is made public in hopes that someone will find it useful, as well as for my own purposes. Every time I make a major change to my server, I update the playbook with more tasks to automate this change, so that I never need to do those tasks again. In order to deploy my server to a new host, I simply need to install CentOS 7 on the machine (usually via an ISO image so I can set up a 1GB swap partition), edit the hosts file on my local machine to point dylanmtaylor to the new IP address, copy my SSH public key to the server using &lt;code&gt;ssh-copy-id root@dylanmtaylor&lt;/code&gt;, run &lt;code&gt;ansible-playbook playbook.yml&lt;/code&gt; and wait. Then when that is done, I can login to the server via SSH or Cockpit and change the password for the users to be more secure. Once that is done, I have a new server that is fully configured and ready to serve my website.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Dylan Taylor&quot;, &quot;login&quot;=&gt;&quot;dylanmtaylor&quot;, &quot;email&quot;=&gt;&quot;dylan@dylanmtaylor.com&quot;}</name><email>dylan@dylanmtaylor.com</email></author><summary type="html">Traditionally, deploying infrastructure was a very time and labor intensive process. Manually going in and configuring services, installing packages, applying system updates, copying over data, etc. could easily take an entire day or more of man hours. In order to solve this problem, several tools, such as Ansible, Chef, Puppet, and SaltStack were created in order to make the lives of system administrators and infrastructure architects everywhere easier. I have adopted Ansible in order to configure my web server. Honestly, the way that my server is configured, there is nothing on it that I cannot afford to lose. With very little effort, I can re-run the Ansible “playbook”, and an identical serer will be created within a couple hours. This is a concept called immutable infrastructure. If there is something wrong with the server, it makes more sense to simply replace it rather than reconfigure or repair it.</summary></entry><entry><title type="html">Rapidly Setting Up My Linux Desktop Using a Makefile</title><link href="https://dylanmtaylor.com/pages/blog/2018/04/27/rapidly-setting-up-my-linux-desktop-using-a-makefile/" rel="alternate" type="text/html" title="Rapidly Setting Up My Linux Desktop Using a Makefile" /><published>2018-04-27T22:49:22+00:00</published><updated>2018-04-27T22:49:22+00:00</updated><id>https://dylanmtaylor.com/pages/blog/2018/04/27/rapidly-setting-up-my-linux-desktop-using-a-makefile</id><content type="html" xml:base="https://dylanmtaylor.com/pages/blog/2018/04/27/rapidly-setting-up-my-linux-desktop-using-a-makefile/">&lt;p&gt;I’ll admit it, I’m obsessed with being an early adopter of technology and I switch between Linux distributions frequently. Before Ubuntu 18.04 came out (it was just released on April 26th), I decided to test it out around the end of March, just a month before it’s official release date. There’s just one issue with that: it wasn’t the final version. Breakage can often happen when testing out a pre-release Linux distibution. For instance, a simply &lt;code&gt;sudo apt update; sudo apt full-upgrade&lt;/code&gt; can cripple a system with a bad package, resulting in a reinstall. I stumbled upon a &lt;a href=&quot;https://gist.github.com/h4cc/c54d3944cb555f32ffdf25a5fa1f2602#file-makefile&quot;&gt;Makefile by Julius Beckmann&lt;/a&gt; that can be run on a new Ubuntu system and immediately saw the benefit of such an approach. I can reinstall as many times as I like, and so long as I set up the commands correctly, I can have a system configured exactly how I like it within an hour. I can even run a specific make target on its own in order to install a certain set of programs or updates without rerunning the whole script. I decided to fork it, creating &lt;a href=&quot;https://github.com/dylanmtaylor/dylan-ubuntu-makefile&quot;&gt;my own Ubuntu Makefile repository&lt;/a&gt;, adding many packages, and removing things I do not need.&lt;/p&gt;

&lt;p&gt;I approached this as a development project at first, running Ubuntu 18.04 daily images in &lt;a href=&quot;https://www.virtualbox.org/&quot;&gt;VirtualBox&lt;/a&gt;. VirtualBox is free and it has a very nice feature called snapshots. I would take a snapshot of a running virtual machine, test out the project, check the final state, and revert to the snapshot before it was applied, so that I could make corrections. After a few days of testing, I was ready to install this on real hardware. I backed up my desktop’s home directory, and installed the 18.04 daily build. As I add packages or software to my system, I make changes to the Makefile if I want that software to be reinstalled when I rebuild my system. Now, I don’t have to worry about wiping and reloading, and it is a huge timesaver. It also allows me to keep the software between my laptop and my desktop relatively in-sync since I can recreate the steps taken perfectly. Now, when I have to reinstall for whatever reason, the only data I have to worry about backing up is the &lt;code&gt;/home/&lt;/code&gt; directory.&lt;/p&gt;

&lt;p&gt;To take this one step further, as I love to test out newer versions than provided in the Ubuntu repositories, I am actually using Flatpak, Snap, PPAs, and some Git repositories to provide software. I’m actually removing the versions of things like LibreOffice and installing the flatpak version instead. As such, I need to check for Flatpak and snap platform updates in order to fully update my machine. Inspired by the &lt;code&gt;zypper dup&lt;/code&gt; command on OpenSUSE, I created an alias called &lt;code&gt;dup&lt;/code&gt;, which can easily update everything, including Flatpaks and snaps:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;alias dup=&quot;sudo apt clean all; sudo apt update; sudo apt -y full-upgrade; sudo flatpak update; sudo snap refresh; sudo apt autoremove&quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I’ve created &lt;a href=&quot;https://gist.github.com/dylanmtaylor/e4176e339e0e1f4c07e5b807cfa9ed9d&quot;&gt;a gist of the common aliases that I use&lt;/a&gt;. I then added this alias to my &lt;code&gt;~/.bash_aliases file&lt;/code&gt; By simply typing &lt;code&gt;dup&lt;/code&gt; into a terminal, I can now get a fully updated system with many bleeding edge packages.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Dylan Taylor&quot;, &quot;login&quot;=&gt;&quot;dylanmtaylor&quot;, &quot;email&quot;=&gt;&quot;dylan@dylanmtaylor.com&quot;}</name><email>dylan@dylanmtaylor.com</email></author><summary type="html">I’ll admit it, I’m obsessed with being an early adopter of technology and I switch between Linux distributions frequently. Before Ubuntu 18.04 came out (it was just released on April 26th), I decided to test it out around the end of March, just a month before it’s official release date. There’s just one issue with that: it wasn’t the final version. Breakage can often happen when testing out a pre-release Linux distibution. For instance, a simply sudo apt update; sudo apt full-upgrade can cripple a system with a bad package, resulting in a reinstall. I stumbled upon a Makefile by Julius Beckmann that can be run on a new Ubuntu system and immediately saw the benefit of such an approach. I can reinstall as many times as I like, and so long as I set up the commands correctly, I can have a system configured exactly how I like it within an hour. I can even run a specific make target on its own in order to install a certain set of programs or updates without rerunning the whole script. I decided to fork it, creating my own Ubuntu Makefile repository, adding many packages, and removing things I do not need.</summary></entry><entry><title type="html">Using Cockpit to Administer Linux Servers in the Browser</title><link href="https://dylanmtaylor.com/pages/blog/2018/04/27/using-cockpit-to-administer-linux-servers-in-the-browser/" rel="alternate" type="text/html" title="Using Cockpit to Administer Linux Servers in the Browser" /><published>2018-04-27T22:25:40+00:00</published><updated>2018-04-27T22:25:40+00:00</updated><id>https://dylanmtaylor.com/pages/blog/2018/04/27/using-cockpit-to-administer-linux-servers-in-the-browser</id><content type="html" xml:base="https://dylanmtaylor.com/pages/blog/2018/04/27/using-cockpit-to-administer-linux-servers-in-the-browser/">&lt;p&gt;It’s often really nice to be able to log into a web interface and check the status of your server and run a few terminal commands. Previously for this, I was using the &lt;a href=&quot;http://ajenti.org/&quot;&gt;Ajenti Server Admin Panel&lt;/a&gt;, but I recently discovered a new solution that I like better because of its simplicity and sponsorship by Red Hat. That solution is the &lt;a href=&quot;http://cockpit-project.org/&quot;&gt;Cockpit project&lt;/a&gt;. As described on the official website, “Cockpit is a server manager that makes it easy to administer your GNU/Linux servers via a web browser.” I decided to give it a try. Since my server is running CentOS 7, this was very easy to get up and running, and it is actually included by default in the CentOS repositories as the &lt;code&gt;cockpit&lt;/code&gt; package. This is a great addition to tools like &lt;code&gt;glances&lt;/code&gt; and &lt;code&gt;htop&lt;/code&gt;, and makes checking on the server a breeze.&lt;/p&gt;

&lt;p&gt;Installing the base package, enabling it, and setting the firewall rules as described on the official website gets you a functional web UI, with things like the ability to login to a terminal session without using SSH, but it’s a bit limited in the default installation. I quickly discovered that cockpit is extendable by modules, and added some additional packages to get more functionality, including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;cockpit-storaged&lt;/li&gt;
  &lt;li&gt;cockpit-networkmanager&lt;/li&gt;
  &lt;li&gt;cockpit-packagekit&lt;/li&gt;
  &lt;li&gt;cockpit-selinux&lt;/li&gt;
  &lt;li&gt;cockpit-pcp&lt;/li&gt;
  &lt;li&gt;cockpit-sosreport&lt;/li&gt;
  &lt;li&gt;cockpit-docker&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once all these packages are installed, I was really satisfied with the end result, so I added this to the Ansible script for the site. Now every time I reinstall, I get cockpit without having to do any additional work. The interface allows me to check things like CPU, RAM, network, and disk utilization, at a glance, and with a single click, I can spawn a terminal interface within the cockpit dashboard. It’s also fantastic for managing &lt;a href=&quot;https://www.docker.com/&quot;&gt;Docker containers&lt;/a&gt;. If you’re an administrator and want a neat way to control your servers, I’d recommend checking it out.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Dylan Taylor&quot;, &quot;login&quot;=&gt;&quot;dylanmtaylor&quot;, &quot;email&quot;=&gt;&quot;dylan@dylanmtaylor.com&quot;}</name><email>dylan@dylanmtaylor.com</email></author><summary type="html">It’s often really nice to be able to log into a web interface and check the status of your server and run a few terminal commands. Previously for this, I was using the Ajenti Server Admin Panel, but I recently discovered a new solution that I like better because of its simplicity and sponsorship by Red Hat. That solution is the Cockpit project. As described on the official website, “Cockpit is a server manager that makes it easy to administer your GNU/Linux servers via a web browser.” I decided to give it a try. Since my server is running CentOS 7, this was very easy to get up and running, and it is actually included by default in the CentOS repositories as the cockpit package. This is a great addition to tools like glances and htop, and makes checking on the server a breeze.</summary></entry><entry><title type="html">Install Google Web Fonts on your Linux or macOS Machine</title><link href="https://dylanmtaylor.com/pages/blog/2018/04/27/install-google-web-fonts-on-your-linux-or-macos-machine/" rel="alternate" type="text/html" title="Install Google Web Fonts on your Linux or macOS Machine" /><published>2018-04-27T22:16:25+00:00</published><updated>2018-04-27T22:16:25+00:00</updated><id>https://dylanmtaylor.com/pages/blog/2018/04/27/install-google-web-fonts-on-your-linux-or-macos-machine</id><content type="html" xml:base="https://dylanmtaylor.com/pages/blog/2018/04/27/install-google-web-fonts-on-your-linux-or-macos-machine/">&lt;p&gt;Google provides a very nice collection of fonts called &lt;a href=&quot;https://fonts.google.com/&quot;&gt;Google Fonts&lt;/a&gt;, with &lt;a href=&quot;https://github.com/google/fonts&quot;&gt;a repository on GitHub&lt;/a&gt;. While looking for fonts to use for personal document creation, I stumbled upon a project called &lt;a href=&quot;https://github.com/qrpike/Web-Font-Load&quot;&gt;Web-Font-Load by Quinton Pike&lt;/a&gt;. I decided to test it out, and it easily installed the collection on my machine. I noticed though that the project had several install/uninstall files that were platform specific. I ended up rewriting the installation script to be common to multiple Linux distributions as well as macOS and committing the changes via &lt;a href=&quot;https://github.com/qrpike/Web-Font-Load/pull/22&quot;&gt;a pull request to the upstream repository&lt;/a&gt;. After the changes that I made, it is now possible to simply run a single command, regardless of your platform, and the fonts will be installed:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;curl https://raw.githubusercontent.com/qrpike/Web-Font-Load/master/install.sh | bash&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This is a very nice collection of over 1500 quality fonts from Google and other contributors, and there are many different styles. It includes fonts you’ll see in things like Google Docs. After running the script, you may need to reboot your system for the fonts to be picked up.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Dylan Taylor&quot;, &quot;login&quot;=&gt;&quot;dylanmtaylor&quot;, &quot;email&quot;=&gt;&quot;dylan@dylanmtaylor.com&quot;}</name><email>dylan@dylanmtaylor.com</email></author><summary type="html">Google provides a very nice collection of fonts called Google Fonts, with a repository on GitHub. While looking for fonts to use for personal document creation, I stumbled upon a project called Web-Font-Load by Quinton Pike. I decided to test it out, and it easily installed the collection on my machine. I noticed though that the project had several install/uninstall files that were platform specific. I ended up rewriting the installation script to be common to multiple Linux distributions as well as macOS and committing the changes via a pull request to the upstream repository. After the changes that I made, it is now possible to simply run a single command, regardless of your platform, and the fonts will be installed:</summary></entry></feed>