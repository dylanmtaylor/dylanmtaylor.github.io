<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.3">Jekyll</generator><link href="https://dylanmtaylor.com/pages/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://dylanmtaylor.com/pages/blog/" rel="alternate" type="text/html" /><updated>2018-06-09T00:43:52+00:00</updated><id>https://dylanmtaylor.com/pages/blog/</id><entry><title type="html">Using Virtual Environments to Avoid Polluting System Python Libraries</title><link href="https://dylanmtaylor.com/pages/blog/2018/06/09/using_virtual_environments_to_avoid_polluting_system_python_libraries/" rel="alternate" type="text/html" title="Using Virtual Environments to Avoid Polluting System Python Libraries" /><published>2018-06-09T00:41:12+00:00</published><updated>2018-06-09T00:41:12+00:00</updated><id>https://dylanmtaylor.com/pages/blog/2018/06/09/using_virtual_environments_to_avoid_polluting_system_python_libraries</id><content type="html" xml:base="https://dylanmtaylor.com/pages/blog/2018/06/09/using_virtual_environments_to_avoid_polluting_system_python_libraries/">&lt;p&gt;As a Python developer, it’s incredibly common to install a new library using the &lt;code&gt;pip&lt;/code&gt; tool. However, if you are running Linux, one of the issues you can get with this is that you can overwrite system python libraries that are managed by a package manager such as &lt;code&gt;apt&lt;/code&gt; or &lt;code&gt;yum&lt;/code&gt;. In addition, if you have several Python projects on your system, each requiring a different version of a dependency, it’s possible that one might break the other. Enter &lt;code&gt;virtualenv&lt;/code&gt;, a tool used to isolate python dependencies within a project (or just in any folder, really). virtualenv allows you to install Python packages in a standard manner using pip, but keeps them separate from the rest of the system.&lt;/p&gt;

&lt;p&gt;Assuming you have Python and &lt;a href=&quot;https://pip.pypa.io/en/stable/installing/&quot;&gt;pip&lt;/a&gt; installed, you can install virtualenv by running this command as root:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pip install virtualenv
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will install the &lt;code&gt;virtualenv&lt;/code&gt; command. Once you have this, you can create a virtual environment for your project. Running the command &lt;code&gt;virtualenv [destination folder]&lt;/code&gt; in will create a folder in the current directory that contains a virtual environment with a complete copy of Python (so you can standardize on one version), as well as a copy of pip which can be used to install any additional packages desired. Advanced use includes having a different version of python included in the environment using the -p flag and the location of the python binary as an argument. Once you have your virtual environment created, you need to source the &lt;code&gt;bin/activate&lt;/code&gt; file within the virtual environment’s folder. Then you can proceed to install whatever you want with &lt;code&gt;pip&lt;/code&gt;. Finally, when you want to leave your virtual environment, you can simply run &lt;code&gt;deactivate&lt;/code&gt;.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Dylan Taylor&quot;, &quot;login&quot;=&gt;&quot;dylanmtaylor&quot;, &quot;email&quot;=&gt;&quot;dylan@dylanmtaylor.com&quot;}</name><email>dylan@dylanmtaylor.com</email></author><summary type="html">As a Python developer, it’s incredibly common to install a new library using the pip tool. However, if you are running Linux, one of the issues you can get with this is that you can overwrite system python libraries that are managed by a package manager such as apt or yum. In addition, if you have several Python projects on your system, each requiring a different version of a dependency, it’s possible that one might break the other. Enter virtualenv, a tool used to isolate python dependencies within a project (or just in any folder, really). virtualenv allows you to install Python packages in a standard manner using pip, but keeps them separate from the rest of the system.</summary></entry><entry><title type="html">Defining Continuous Integration and Continuous Delivery</title><link href="https://dylanmtaylor.com/pages/blog/2018/06/08/defining_continuous_integration_and_continuous_delivery/" rel="alternate" type="text/html" title="Defining Continuous Integration and Continuous Delivery" /><published>2018-06-08T19:23:06+00:00</published><updated>2018-06-08T19:23:06+00:00</updated><id>https://dylanmtaylor.com/pages/blog/2018/06/08/defining_continuous_integration_and_continuous_delivery</id><content type="html" xml:base="https://dylanmtaylor.com/pages/blog/2018/06/08/defining_continuous_integration_and_continuous_delivery/">&lt;p&gt;In the past, I’ve been asked what continuous integration and continuous delivery mean to me. Those not used to the DevOps Engineering methodology, which integrates traditional application development with infrastructure operations in order to expedite the production code delivery process and improve iteration time in an agile manner may not be familiar with these terms. Unlike the word agile, which is overused to the point where it has become little more than a marketing buzzword in some cases, CI/CD refer to concretely define processes. The implementation may vary, of course, for instance, a CI/CD workflow could take advantage of a hosted solution (software as a service) such as TravisCI, or IBM Bluemix DevOps Services (a product I’ve worked on), or it could run within one’s own infrastructure.&lt;/p&gt;

&lt;p&gt;Integration testing is defined as testing the software modules as a group, and tends to mean testing the product as a whole instead of specific functionality, unlike unit tests which can be at the function level. Continuous integration is a practice where integration testing is done frequently, perhaps multiple times in a single day. For instance, when members of the team developing the product commit their work to a repository, via a pull request (called a merge request in GitLab), an automated build process is initiated. It could instantiate a Docker container or Jenkins job in order to build the product with the revised source code, but the specific solution doesn’t matter in this case. Once the product is built, the test suite is run against it and feedback is provided via predefined channels in a way that errors can be quickly detected so that bugs are not introduced into the codebase.&lt;/p&gt;

&lt;p&gt;Continuous delivery takes this one step further. Once continuous integration is in place, it’s just one of the pieces of a continuous delivery workflow. CD takes advantage of the constant testing of the product and allows the development changes to quickly get pushed from development to a test environment, and then to production. This typically involves what’s called a ‘delivery pipeline’, which is a service that orchestrates the promotion of code from one ‘stage’ to another. Each stage represents an element of the delivery process, such as building or deploying to an environment. Continuous delivery strongly relies on a good delivery pipeline. In a proper setup, new product features and bug fixes can be built and deployed from one stage to another rapidly without accepting a great deal of risk. However, it is of utmost importance that there are sufficient integration tests as well as good code coverage by unit tests in the product in order for this process to work effectively. By taking advantage of a CI/CD workflow, either using off the shelf tools or implementing a custom solution, it is possible to significantly speed up the code delivery process and reduce ceremony.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Dylan Taylor&quot;, &quot;login&quot;=&gt;&quot;dylanmtaylor&quot;, &quot;email&quot;=&gt;&quot;dylan@dylanmtaylor.com&quot;}</name><email>dylan@dylanmtaylor.com</email></author><summary type="html">In the past, I’ve been asked what continuous integration and continuous delivery mean to me. Those not used to the DevOps Engineering methodology, which integrates traditional application development with infrastructure operations in order to expedite the production code delivery process and improve iteration time in an agile manner may not be familiar with these terms. Unlike the word agile, which is overused to the point where it has become little more than a marketing buzzword in some cases, CI/CD refer to concretely define processes. The implementation may vary, of course, for instance, a CI/CD workflow could take advantage of a hosted solution (software as a service) such as TravisCI, or IBM Bluemix DevOps Services (a product I’ve worked on), or it could run within one’s own infrastructure.</summary></entry><entry><title type="html">How to Install the Latest Master Branch Build of Slic3r</title><link href="https://dylanmtaylor.com/pages/blog/2018/05/17/how_to_install_the_latest_master_branch_build_of_slic3r/" rel="alternate" type="text/html" title="How to Install the Latest Master Branch Build of Slic3r" /><published>2018-05-17T01:22:15+00:00</published><updated>2018-05-17T01:22:15+00:00</updated><id>https://dylanmtaylor.com/pages/blog/2018/05/17/how_to_install_the_latest_master_branch_build_of_slic3r</id><content type="html" xml:base="https://dylanmtaylor.com/pages/blog/2018/05/17/how_to_install_the_latest_master_branch_build_of_slic3r/">&lt;p&gt;&lt;a href=&quot;http://slic3r.org/&quot;&gt;Slic3r&lt;/a&gt; is a 3D printer slicing tool. It converts digital objects into instructions that are readable by a 3D printer called “gcode”. I use this program with my 3D printer, an Original Prusa i3 MK2s in order to position, rotate, and scale objects on the print bed and then select how I would like it to be printed. It’s an excellent tool for the most part, and it competes quite well with the proprietary tool &lt;a href=&quot;https://www.simplify3d.com/&quot;&gt;Simplify3D&lt;/a&gt;. Improvements seem to be added quite frequently. However, I have noticed that for some reason, the &lt;code&gt;slic3r&lt;/code&gt; package on Ubuntu 18.04 breaks when running it with the Nvidia proprietary graphics driver installed, but this does not affect the slic3r appimage package or upstream tarball releases. In order to solve this issue, you can easily get the files, extract them and create a menu entry using this method:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget -N https://dl.slic3r.org/dev/linux/Slic3r-master-latest.tar.bz2
sudo rm -rf /opt/Slic3r/
sudo tar xvjf Slic3r-master-latest.tar.bz2 -C /opt/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This puts a self-contained installation of the bleeding-edge version of Slic3r in the /opt directory. You can then put the following into &lt;code&gt;/usr/share/applications/slic3r_master.desktop&lt;/code&gt; to add a desktop icon:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Desktop Entry]
Version=1.0
Type=Application
Name=Slic3r
Icon=/opt/Slic3r/var/Slic3r.png
Exec=bash -c 'cd /opt/Slic3r &amp;amp;&amp;amp; /opt/Slic3r/Slic3r --gui %F'
Keywords=perl;slice;3D;printer;convert;gcode;stl;obj;amf;
StartupNotify=false
StartupWMClass=slic3r
MimeType=application/sla;model/x-wavefront-obj;model/x-geomview-off;application/x-amf;
Categories=Development;Engineering;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you’re like me and you’re using the wonderful &lt;a href=&quot;https://github.com/PapirusDevelopmentTeam/papirus-icon-theme&quot;&gt;Papirus icon theme&lt;/a&gt; or another theme providing the icon, editing the “Icon” line to be just “Icon=slic3r” will give you a properly themed icon for Slic3r since the icon path is no longer hardcoded and your desktop environment will look for the icon under &lt;code&gt;/usr/share/icons/&lt;/code&gt;. You should now be able to start slic3r without any issues, even with the driver installed. This is also great for testing out new unstable features and finding bugs, if you’re into that sort of thing.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Dylan Taylor&quot;, &quot;login&quot;=&gt;&quot;dylanmtaylor&quot;, &quot;email&quot;=&gt;&quot;dylan@dylanmtaylor.com&quot;}</name><email>dylan@dylanmtaylor.com</email></author><summary type="html">Slic3r is a 3D printer slicing tool. It converts digital objects into instructions that are readable by a 3D printer called “gcode”. I use this program with my 3D printer, an Original Prusa i3 MK2s in order to position, rotate, and scale objects on the print bed and then select how I would like it to be printed. It’s an excellent tool for the most part, and it competes quite well with the proprietary tool Simplify3D. Improvements seem to be added quite frequently. However, I have noticed that for some reason, the slic3r package on Ubuntu 18.04 breaks when running it with the Nvidia proprietary graphics driver installed, but this does not affect the slic3r appimage package or upstream tarball releases. In order to solve this issue, you can easily get the files, extract them and create a menu entry using this method:</summary></entry><entry><title type="html">Using Vagrant Shell Provisioning to Execute an Isolated Build Process in a VM</title><link href="https://dylanmtaylor.com/pages/blog/2018/05/16/using_vagrant_to_execute_an_isolated_build_process_in_a_vm/" rel="alternate" type="text/html" title="Using Vagrant Shell Provisioning to Execute an Isolated Build Process in a VM" /><published>2018-05-16T22:23:38+00:00</published><updated>2018-05-16T22:23:38+00:00</updated><id>https://dylanmtaylor.com/pages/blog/2018/05/16/using_vagrant_to_execute_an_isolated_build_process_in_a_vm</id><content type="html" xml:base="https://dylanmtaylor.com/pages/blog/2018/05/16/using_vagrant_to_execute_an_isolated_build_process_in_a_vm/">&lt;p&gt;&lt;a href=&quot;https://www.vagrantup.com/docs/provisioning/shell.html&quot;&gt;Vagrant Shell provisioning&lt;/a&gt; is an incredibly useful tool that can be utilized to run any arbitraty code within a virtual machine. It’s one of the fundamental ways of using Vagrant, and is by far one of the simplest. &lt;a href=&quot;https://app.vagrantup.com/boxes/search&quot;&gt;Vagrant boxes&lt;/a&gt; are preconfigured virtual machine images, usually either a bare operating system or with some sort of software installed, for instance a LAMP stack. You can do anything you can do on a regular virtual machine with a Vagrant installed one, and Vagrant also allows you to do arbitrary port forwarding from the virtual machine to the host. This is incredibly useful in the case of wanting to deploy software to a virtual machine without having to configure it, and is great for development and testing. Another use that I learned of is using Vagrant to run a build process on a version of an operating system different than the host OS on your development workstation, and then pulling the built files off of the VM via SSH. This could be useful for a packaging scenario. Since a single Vagrantfile can contain multiple VMs, you can use this to target, for example a few Linux distributions (e.g. Ubuntu, RHEL, SLES) and Windwows all in a single action. This could speed up the testing process, as each virtual machine would automatically be able to deploy and run your code and you can run integration tests against them concurrently, perhaps by forwarding each VM to a different network port, in order to do software quality assurance.&lt;/p&gt;

&lt;p&gt;A very basic example using &lt;a href=&quot;https://www.virtualbox.org/&quot;&gt;VirtualBox&lt;/a&gt; as a hypervisor looks like the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Vagrantfile API/syntax version. Don't touch unless you know what you're doing!
VAGRANTFILE_API_VERSION = &quot;2&quot;

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|

  # Here we configure the resources allocated to the virtual machine
  config.vm.provider :virtualbox do |vb|
    vb.customize [&quot;modifyvm&quot;, :id, &quot;--memory&quot;, &quot;2048&quot;]
    vb.customize [&quot;modifyvm&quot;, :id, &quot;--cpus&quot;, &quot;4&quot;]
  end

  # This section tells Vagrant where to find the virtual machine image, and what ports to forward
  # The ubuntu.vm.provision line tells the VM to execute the shell script found on the defined path
  config.vm.define &quot;ubuntu&quot;, primary: true do |ubuntu|
    ubuntu.vm.network :forwarded_port, guest: 80, host: 8080
    ubuntu.vm.box = &quot;ubuntu/xenial64&quot;
    ubuntu.ssh.forward_agent = true
    ubuntu.vm.provision &quot;shell&quot;, path:&quot;scripts/ubuntu_provision.sh&quot;
  end

  # Additional build targets can be defined here, e.g. RHEL, Windows Server.

end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this case, the shell target is relative to the current working directory in the project. An old friend of mine recently wanted to try playing with Wordpress, and asked me for help setting it up. As an example here, because writing software to test is outside the scope of this post, I will be using Vagrant to deploy Wordpress to Ubuntu. In order to do this, I’ll be using &lt;a href=&quot;https://make.wordpress.org/cli/handbook/quick-start/&quot;&gt;the WP-cli tool&lt;/a&gt;. The file &lt;code&gt;scripts/ubuntu_provision.sh&lt;/code&gt; from the Vagrantfile will look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash

# Install LAMP Server non-interactively
DEBIAN_FRONTEND=noninteractive apt-get -q -y install lamp-server^

# Cleanup and install WP cli tool
rm -rf /var/www/html/* # remove existing Apache files
cd /var/www/html
curl -O https://raw.githubusercontent.com/wp-cli/builds/gh-pages/phar/wp-cli.phar
chmod +x wp-cli.phar
sudo mv wp-cli.phar /usr/local/bin/wp

# Disable MySQL security for simplicity here; a proper installation would be configured with mysql_secure_installation
systemctl stop mysql.service
mkdir -p /var/run/mysqld; chown mysql:mysql /var/run/mysqld; mysqld_safe --skip-grant-tables &amp;amp;

# Setup Wordpress
wp --allow-root core download
mysql -uroot -e &quot;CREATE DATABASE wordpressdb; GRANT ALL PRIVILEGES ON wordpressdb.* TO 'wordpressuser'@'%' IDENTIFIED BY 'password'; FLUSH PRIVILEGES;&quot;
wp --allow-root config create --dbname=wordpressdb --dbuser=wordpresssuser --dbpass=password --dbhost=127.0.0.1 # Generate wp-config.php
wp --allow-root core install --url=localhost --title=&quot;Example&quot; --admin_user=admin --admin_password=Passw0rd --admin_email=me@myself.org

# Handle services
systemctl enable mysql
systemctl reload apache2

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because of the &lt;a href=&quot;https://www.vagrantup.com/docs/networking/forwarded_ports.html&quot;&gt;forwarded port configuration&lt;/a&gt;, this will enable the Wordpress installation on the guest to be accessible via port 8080 on the host machine. To test this, you can create the file structure described, run &lt;code&gt;vagrant up&lt;/code&gt; in the directory containing the Vagrant file, and browse to &lt;a href=&quot;http://localhost:8080&quot;&gt;localhost in your browser&lt;/a&gt;. If you decide to use this, please keep in mind that this dev install of wordpress is insecure in many ways.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Dylan Taylor&quot;, &quot;login&quot;=&gt;&quot;dylanmtaylor&quot;, &quot;email&quot;=&gt;&quot;dylan@dylanmtaylor.com&quot;}</name><email>dylan@dylanmtaylor.com</email></author><summary type="html">Vagrant Shell provisioning is an incredibly useful tool that can be utilized to run any arbitraty code within a virtual machine. It’s one of the fundamental ways of using Vagrant, and is by far one of the simplest. Vagrant boxes are preconfigured virtual machine images, usually either a bare operating system or with some sort of software installed, for instance a LAMP stack. You can do anything you can do on a regular virtual machine with a Vagrant installed one, and Vagrant also allows you to do arbitrary port forwarding from the virtual machine to the host. This is incredibly useful in the case of wanting to deploy software to a virtual machine without having to configure it, and is great for development and testing. Another use that I learned of is using Vagrant to run a build process on a version of an operating system different than the host OS on your development workstation, and then pulling the built files off of the VM via SSH. This could be useful for a packaging scenario. Since a single Vagrantfile can contain multiple VMs, you can use this to target, for example a few Linux distributions (e.g. Ubuntu, RHEL, SLES) and Windwows all in a single action. This could speed up the testing process, as each virtual machine would automatically be able to deploy and run your code and you can run integration tests against them concurrently, perhaps by forwarding each VM to a different network port, in order to do software quality assurance.</summary></entry><entry><title type="html">Playing with Kubernetes for the First Time</title><link href="https://dylanmtaylor.com/pages/blog/2018/05/05/playing_with_kubernetes/" rel="alternate" type="text/html" title="Playing with Kubernetes for the First Time" /><published>2018-05-05T02:18:37+00:00</published><updated>2018-05-05T02:18:37+00:00</updated><id>https://dylanmtaylor.com/pages/blog/2018/05/05/playing_with_kubernetes</id><content type="html" xml:base="https://dylanmtaylor.com/pages/blog/2018/05/05/playing_with_kubernetes/">&lt;p&gt;&lt;a href=&quot;https://kubernetes.io/&quot;&gt;&lt;img src=&quot;/images/blog/2018/05/kubernetes.svg&quot; width=&quot;150&quot; height=&quot;150&quot; style=&quot;float:left&quot; /&gt;&lt;/a&gt; For a while now, I’ve been interested in learning a container orchestration technology called &lt;a href=&quot;https://kubernetes.io/&quot;&gt;Kubernetes&lt;/a&gt;, which was developed by Google and licensed under the Apache License 2.0. I’ve been working through the &lt;a href=&quot;https://www.katacoda.com/courses/kubernetes&quot;&gt;“Learn Kubernetes using Interactive Browser-Based Scenarios
“&lt;/a&gt; on Katacoda, which is a free online way to learn some new technology without a large personal investment. I wanted to play around without the constraints of a course, so I decided to spend a few dollars on bare-metal hosting resources on &lt;a href=&quot;https://www.packet.net/&quot;&gt;packet.net&lt;/a&gt;. They gave me $25 worth of free introductory credit so I didn’t even pay anything for this exercise. Packet.net has an offering called &lt;code&gt;t1.small.x86&lt;/code&gt; with 8GB RAM, 80GB disk (the one I ordered actually came with a 150GB Intel SSDSC2BB15), and an Intel Atom C2550 processor. I ordered one in &lt;a href=&quot;http://www.parsippany.net/&quot;&gt;Parsippany, NJ&lt;/a&gt; as that was the closest location to me. Bandwidth is not limited, but it is billed at $0.05/GB outgoing, which wasn’t an issue. Honestly, the power of dedicated servers is quite remarkable. I previously hosted dylanmtaylor.com on a Joe’s Datacenter dual Xeon machine, and it had great performance. Compared to that, the HostHatch NVMe I have my site running on right now is quite underpowered. That said, the value is certainly there, and it’s been very reliable so far. Anyways, let’s start playing with Kubernetes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://coreos.com/&quot;&gt;CoreOS&lt;/a&gt; was a natural fit for this project. I’ve heard a lot about it, including when I attended the &lt;a href=&quot;https://allthingsopen.org/&quot;&gt;All Things Open&lt;/a&gt; conference in Raleigh, NC and met with some of the people working on it. At work I’ve used &lt;a href=&quot;https://www.docker.com/&quot;&gt;Docker&lt;/a&gt; with the &lt;a href=&quot;https://docs.docker.com/ee/dtr/&quot;&gt;Docker Trusted Registry&lt;/a&gt; product quite a bit and realized the power it has to simplify deployments and server as a host for running continuous integration jobs, so I decided to dive right in and rather than installing &lt;a href=&quot;https://www.centos.org/&quot;&gt;CentOS&lt;/a&gt; like I usually would and installing Docker on top of that, I’d just install CoreOS as a base operating system. I’m still not sure what “Container Linux” means, but it feels like a solid foundation for exploration. From what I’ve seen so far, it’s basically a stable Linux foundation that is “stripped down” compared to more general purpose distributions. It’s purpose built to use very few resources and just get out of the way. Fortunately with packet.net, I didn’t have to do any of the installation work myself, as they provided an OS template that is installed via iPXE on initial bootup.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2018/05/coreos_neofetch.png&quot; alt=&quot;neofetch&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The first thing I did once the server was installed was SSH in and check the hardware out. I also took the opportunity to curl &lt;code&gt;neofetch&lt;/code&gt; and run that to get a fancy screenshot for this blog post. I then started reading up on the &lt;a href=&quot;https://kubernetes.io/docs/home/&quot;&gt;documentation for Kubernetes&lt;/a&gt; and the &lt;a href=&quot;https://coreos.com/tectonic/docs/latest/tutorials/kubernetes/getting-started.html&quot;&gt;CoreOS + Kubernetes docs&lt;/a&gt;. For the time being, I decided against using Tectonic, as it looks like it is overkill for just testing out Kubernetes, and it looks like a paid product (although I was able to find source code for it). I checked the internet and found &lt;a href=&quot;https://adminswerk.de/kubernetes-coreos-single-node/&quot;&gt;a post  on Kubernetes and CoreOS on a single node&lt;/a&gt; from someone else who went through the same exercise. They took a script &lt;a href=&quot;https://victorpalau.net/2016/09/04/single-node-kubernetes-deployment/&quot;&gt;originally by Victor Palau&lt;/a&gt; and modified it to be more secure and less reliant on Microsoft Azure. After reviewing the &lt;code&gt;kubeform.sh&lt;/code&gt; code from &lt;a href=&quot;https://github.com/m3adow/k8single&quot;&gt;the GitHub repository&lt;/a&gt;, I decided to test it out on my server. As I was already connected via SSH, I simply had to execute the commands in the &lt;code&gt;README.md&lt;/code&gt; file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/m3adow/k8single/; 
cd k8single
./kubeform.sh [myip-address]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unfortunately, I got an error stating &lt;code&gt;Failed to start etcd2.service: Unit etcd2.service not found&lt;/code&gt;, so I had to install etcd and get that up and running. Or so I thought, until I discovered that &lt;a href=&quot;https://github.com/coreos/coreos-cloudinit/blob/master/Documentation/cloud-config.md&quot;&gt;these services were deprecated&lt;/a&gt;. Fortunately, I managed to find a blog post &lt;a href=&quot;https://coreos.com/blog/toward-etcd-v3-in-container-linux.html&quot;&gt;explaining how to setup etcd v3&lt;/a&gt; and I was able to start and enable &lt;code&gt;etcd-member.service&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;coreostesting core # systemctl enable /usr/lib/systemd/system/etcd-member.service
Created symlink /etc/systemd/system/multi-user.target.wants/etcd-member.service → /usr/lib/systemd/system/etcd-member.service.
coreostesting core # systemctl start etcd-member
coreostesting core # rkt list
UUID		APP	IMAGE NAME			STATE	CREATED		STARTED		NETWORKS
3de95472	etcd	quay.io/coreos/etcd:v3.2.15	running	24 seconds ago	24 seconds ago
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Clearly, &lt;code&gt;etcd-member&lt;/code&gt; was the right service, and &lt;code&gt;etcd2&lt;/code&gt; is deprecated and removed. Progress! As I learned all I know so far from Katacoda, I decided to try to setup the environment similar to what they had. In particular, I wanted to start by getting &lt;code&gt;kubectl&lt;/code&gt; installed on my local system. I was able to track down &lt;a href=&quot;https://kubernetes.io/docs/tasks/tools/install-kubectl/&quot;&gt;a tutorial on installing kubectl&lt;/a&gt; and the instructions were straight-forward. As I am currently running Ubuntu, I used the snap package that was installed with a simple &lt;code&gt;sudo snap install kubectl --classic&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dtaylor@Dylan-Precision-5510:~$ kubectl version
Client Version: version.Info{Major:&quot;1&quot;, Minor:&quot;8&quot;, GitVersion:&quot;v1.8.4&quot;, GitCommit:&quot;9befc2b8928a9426501d3bf62f72849d5cbcd5a3&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2017-11-20T05:28:34Z&quot;, GoVersion:&quot;go1.8.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;}
The connection to the server localhost:8080 was refused - did you specify the right host or port?
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Of course, it points to localhost by default. It looked rather tricky to make it use the CoreOS server, so I used the curl method to download it onto CoreOS directly. The filesystem was read-only so I kept it in the home directory. I really wanted to get the &lt;a href=&quot;https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/&quot;&gt;Kubernetes Web UI&lt;/a&gt; up and running. Unfortunately, this didn’t yield any better results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;core@coreostesting ~ $ ./kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
W0505 01:03:08.989428    1517 factory_object_mapping.go:423] Failed to download OpenAPI (Get http://localhost:8080/swagger-2.0.0.pb-v1: dial tcp [::1]:8080: getsockopt: connection refused), falling back to swagger
The connection to the server localhost:8080 was refused - did you specify the right host or port?
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I searched for this error and was able to find &lt;a href=&quot;https://github.com/kubernetes/kubernetes/issues/44665&quot;&gt;a GitHub issues page&lt;/a&gt; describing the error. It suggested I follow these steps:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo cp /etc/kubernetes/admin.conf $HOME/
sudo chown $(id -u):$(id -g) $HOME/admin.conf
export KUBECONFIG=$HOME/admin.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sadly, the commands failed, with the source files not existing. After some searching, it seemed like I needed to get Kubelet working. Since the newer documentation relied on AWS heavily, I decided to try &lt;a href=&quot;https://coreos.com/blog/introducing-the-kubelet-in-coreos.html&quot;&gt;the older version&lt;/a&gt;. This had some formatting issues so I tried the &lt;a href=&quot;https://github.com/kubernetes/contrib/blob/master/init/systemd/kubelet.service&quot;&gt;kublet.service from GitHub&lt;/a&gt;. After pasting that in, I ran &lt;code&gt;sudo systemctl daemon-reload; sudo systemctl start kubelet&lt;/code&gt;, which didn’t return any output but didn’t work either.&lt;/p&gt;

&lt;p&gt;I decided to scrap CoreOS for now after I managed to stumble upon &lt;a href=&quot;https://blog.alexellis.io/kubernetes-in-10-minutes/&quot;&gt;a blog post about setting up Kubernetes on Packet&lt;/a&gt;, which I read and found particularly detailed and helpful. It also included &lt;a href=&quot;https://pbs.twimg.com/media/DBzjTTKUIAA1OvE.jpg:large&quot;&gt;a great image by Julia Evans showing how all the pieces fit together&lt;/a&gt;. I initiated an OS reload action on the server and reprovisioned it with Ubuntu 16.04 LTS. Packet has a really nice feature called Out-of-Band console which you can use to watch a server get deployed in realtime. Once my server finished provisioning, I got to work. I won’t bore you with too many details, as Alex goes into great detail on his blog, but first, I SSH’d into the newly installed machine as root and turned off swap using &lt;code&gt;swapoff -a&lt;/code&gt; and ran the &lt;a href=&quot;https://gist.githubusercontent.com/alexellis/7315e75635623667c32199368aa11e95/raw/b025dfb91b43ea9309ce6ed67e24790ba65d7b67/kube.sh&quot;&gt;all-in-one dependency installation script&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Once all the dependencies were installed, I ran &lt;code&gt;sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=[ip address] --kubernetes-version stable-1.10&lt;/code&gt;, where &lt;code&gt;[ip address&lt;/code&gt; is the &lt;em&gt;private&lt;/em&gt; bond0 address on the physical machine. While not strictly necessary, I created a non-privileged dylan user as done in the tutorial, and copied the configuration.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo cp /etc/kubernetes/admin.conf $HOME/
sudo chown $(id -u):$(id -g) $HOME/admin.conf
export KUBECONFIG=$HOME/admin.conf

echo &quot;export KUBECONFIG=$HOME/admin.conf&quot; | tee -a ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once that was done, I was able to configure networking using &lt;code&gt;kubectl&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dylan@coreostesting:/root$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
clusterrole.rbac.authorization.k8s.io &quot;flannel&quot; created
clusterrolebinding.rbac.authorization.k8s.io &quot;flannel&quot; created
serviceaccount &quot;flannel&quot; created
configmap &quot;kube-flannel-cfg&quot; created
daemonset.extensions &quot;kube-flannel-ds&quot; created
dylan@coreostesting:/root$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/k8s-manifests/kube-flannel-rbac.yml
clusterrole.rbac.authorization.k8s.io &quot;flannel&quot; configured
clusterrolebinding.rbac.authorization.k8s.io &quot;flannel&quot; configured
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I then ran &lt;code&gt;kubectl taint nodes --all node-role.kubernetes.io/master-&lt;/code&gt; so that I could run this all on a single machine (a real deployment would have many more). Amazingly, the output of &lt;code&gt;kubectl get all --namespace=kube-system&lt;/code&gt; seemed to be working perfectly. Perhaps I’ll revisit CoreOS again, but this was going much smoother. I skipped over much of the rest of the tutorial and decided to try to get the Web UI working. I executed the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then I returned to the blog post and started up the &lt;code&gt;kubectl proxy&lt;/code&gt; on the bare-metal machine and tunneled through the machine on my local host, this time usinig teh &lt;em&gt;public&lt;/em&gt; IP address of the machine. The command looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh -L 8001:127.0.0.1:8001 -N root@[public ip]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unfortunately when I loaded the URL for the localhost:8001 from the post &lt;code&gt;http://localhost:8001/ui/&lt;/code&gt;, I got a wall of text:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &quot;paths&quot;: [
    &quot;/apis&quot;,
    &quot;/apis/&quot;,
    &quot;/apis/apiextensions.k8s.io&quot;,
    &quot;/apis/apiextensions.k8s.io/v1beta1&quot;,
    &quot;/healthz&quot;,
    &quot;/healthz/etcd&quot;,
    &quot;/healthz/ping&quot;,
    &quot;/healthz/poststarthook/generic-apiserver-start-informers&quot;,
    &quot;/healthz/poststarthook/start-apiextensions-controllers&quot;,
    &quot;/healthz/poststarthook/start-apiextensions-informers&quot;,
    &quot;/metrics&quot;,
    &quot;/openapi/v2&quot;,
    &quot;/swagger-2.0.0.json&quot;,
    &quot;/swagger-2.0.0.pb-v1&quot;,
    &quot;/swagger-2.0.0.pb-v1.gz&quot;,
    &quot;/swagger.json&quot;,
    &quot;/swaggerapi&quot;,
    &quot;/version&quot;
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I tracked down &lt;a href=&quot;https://github.com/kubernetes/dashboard&quot;&gt;the GitHub repository for the dashboard&lt;/a&gt;, and realized that the URL needed to be &lt;code&gt;http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/&lt;/code&gt;. I finally had it mostly working. I was able to access the Web UI at least. I decided to tear down the server as it is quite pricey to keep online for more than several hours at a time. Next time I will explore setting up something without using cloud resources, and explore running a containerized workload on top of the Kubernetes software.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Dylan Taylor&quot;, &quot;login&quot;=&gt;&quot;dylanmtaylor&quot;, &quot;email&quot;=&gt;&quot;dylan@dylanmtaylor.com&quot;}</name><email>dylan@dylanmtaylor.com</email></author><summary type="html">For a while now, I’ve been interested in learning a container orchestration technology called Kubernetes, which was developed by Google and licensed under the Apache License 2.0. I’ve been working through the “Learn Kubernetes using Interactive Browser-Based Scenarios “ on Katacoda, which is a free online way to learn some new technology without a large personal investment. I wanted to play around without the constraints of a course, so I decided to spend a few dollars on bare-metal hosting resources on packet.net. They gave me $25 worth of free introductory credit so I didn’t even pay anything for this exercise. Packet.net has an offering called t1.small.x86 with 8GB RAM, 80GB disk (the one I ordered actually came with a 150GB Intel SSDSC2BB15), and an Intel Atom C2550 processor. I ordered one in Parsippany, NJ as that was the closest location to me. Bandwidth is not limited, but it is billed at $0.05/GB outgoing, which wasn’t an issue. Honestly, the power of dedicated servers is quite remarkable. I previously hosted dylanmtaylor.com on a Joe’s Datacenter dual Xeon machine, and it had great performance. Compared to that, the HostHatch NVMe I have my site running on right now is quite underpowered. That said, the value is certainly there, and it’s been very reliable so far. Anyways, let’s start playing with Kubernetes.</summary></entry><entry><title type="html">Using BackBlaze B2 Cloud Storage for Encrypted Offsite Backups</title><link href="https://dylanmtaylor.com/pages/blog/2018/05/04/using_backblaze_b2_cloud_storage_for_encrypted_backups/" rel="alternate" type="text/html" title="Using BackBlaze B2 Cloud Storage for Encrypted Offsite Backups" /><published>2018-05-04T01:04:33+00:00</published><updated>2018-05-04T01:04:33+00:00</updated><id>https://dylanmtaylor.com/pages/blog/2018/05/04/using_backblaze_b2_cloud_storage_for_encrypted_backups</id><content type="html" xml:base="https://dylanmtaylor.com/pages/blog/2018/05/04/using_backblaze_b2_cloud_storage_for_encrypted_backups/">&lt;p&gt;As of right now, one of the cheapest offsite backup solutions by far is &lt;a href=&quot;https://www.backblaze.com/b2/cloud-storage.html&quot;&gt;BackBlaze B2&lt;/a&gt;, with &lt;a href=&quot;https://aws.amazon.com/s3/&quot;&gt;Amazon S3&lt;/a&gt; trailing behind. For a while now, I’ve been using this solution with a piece of software called &lt;a href=&quot;https://restic.net/&quot;&gt;restic&lt;/a&gt; to store encrypted backups of some of my important documents offsite. By doing so, should my Western Digital 8TB external hard drive (which I currently use for all of my backups) fail, or get stolen (it’s strongly encrypted, so my data is safe), I have a solution to get the files I need the most back. While I store anything and everything on my external, I am limiting the files in the offsite backup to things that are important. It’s not economical to push things like your music/movie collection, Linux ISOs and virtual machine hard drive images to the BackBlaze cloud. Ubuntu’s repositories contain the restic binaries, but other distributions may require you to install a new repository or build from source. I could write up a guide on how to set this up, but &lt;a href=&quot;https://fedoramagazine.org/&quot;&gt;Fedora Magazine&lt;/a&gt; wrote &lt;a href=&quot;https://fedoramagazine.org/use-restic-encrypted-backups/&quot;&gt;an excellent article on how to setup this exact solution&lt;/a&gt;.  I’d be doing anyone reading this a disservice trying to write up an article that better explains the setup steps. If you’re not a huge fan of restic, &lt;a href=&quot;http://duplicity.nongnu.org/&quot;&gt;duplicity&lt;/a&gt; also supports BackBlaze B2, and is fairly easy to setup from what I hear.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Dylan Taylor&quot;, &quot;login&quot;=&gt;&quot;dylanmtaylor&quot;, &quot;email&quot;=&gt;&quot;dylan@dylanmtaylor.com&quot;}</name><email>dylan@dylanmtaylor.com</email></author><summary type="html">As of right now, one of the cheapest offsite backup solutions by far is BackBlaze B2, with Amazon S3 trailing behind. For a while now, I’ve been using this solution with a piece of software called restic to store encrypted backups of some of my important documents offsite. By doing so, should my Western Digital 8TB external hard drive (which I currently use for all of my backups) fail, or get stolen (it’s strongly encrypted, so my data is safe), I have a solution to get the files I need the most back. While I store anything and everything on my external, I am limiting the files in the offsite backup to things that are important. It’s not economical to push things like your music/movie collection, Linux ISOs and virtual machine hard drive images to the BackBlaze cloud. Ubuntu’s repositories contain the restic binaries, but other distributions may require you to install a new repository or build from source. I could write up a guide on how to set this up, but Fedora Magazine wrote an excellent article on how to setup this exact solution. I’d be doing anyone reading this a disservice trying to write up an article that better explains the setup steps. If you’re not a huge fan of restic, duplicity also supports BackBlaze B2, and is fairly easy to setup from what I hear.</summary></entry><entry><title type="html">Awesome New Share Bar Added to My Blog</title><link href="https://dylanmtaylor.com/pages/blog/2018/05/04/awesome_new_share_bar_added_to_my_blog/" rel="alternate" type="text/html" title="Awesome New Share Bar Added to My Blog" /><published>2018-05-04T00:44:21+00:00</published><updated>2018-05-04T00:44:21+00:00</updated><id>https://dylanmtaylor.com/pages/blog/2018/05/04/awesome_new_share_bar_added_to_my_blog</id><content type="html" xml:base="https://dylanmtaylor.com/pages/blog/2018/05/04/awesome_new_share_bar_added_to_my_blog/">&lt;p&gt;For the longest time, I’ve had no social media sharing buttons on this site other than a &lt;a href=&quot;https://plus.google.com/&quot;&gt;Google Plus&lt;/a&gt; button. I decided to remove the Disqus comments section from the site since nobody was using it anyways, and realized that I could take the opportunity to update the interface. I searched the web for &lt;a href=&quot;https://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt; social media share buttons and came across &lt;a href=&quot;https://mycyberuniverse.com/web/social-media-share-bar-jekyll-blog-website.html&quot;&gt;an amazing blog post explaining how to add them with some sample code&lt;/a&gt;. After following the tutorial and modifying the HTML and CSS a little bit to better fit with how I want the buttons to be displayed, I am pleased to say that you can now click the share buttons to quickly post a link to any of my content on many different platforms and even email it. I’ve got support for Facebook, Twitter, Reddit, Google Plus, Tumblr and LinkedIn enabled. One feature of this solution that I really like is that the buttons have icons on them from the &lt;a href=&quot;https://fontawesome.com/&quot;&gt;Font Awesome&lt;/a&gt; project. These are all vector social logos that should render quite nicely on any modern browser. Feel free to try it out below if you’d like. It will prompt you to edit the post before sharing it.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Dylan Taylor&quot;, &quot;login&quot;=&gt;&quot;dylanmtaylor&quot;, &quot;email&quot;=&gt;&quot;dylan@dylanmtaylor.com&quot;}</name><email>dylan@dylanmtaylor.com</email></author><summary type="html">For the longest time, I’ve had no social media sharing buttons on this site other than a Google Plus button. I decided to remove the Disqus comments section from the site since nobody was using it anyways, and realized that I could take the opportunity to update the interface. I searched the web for Jekyll social media share buttons and came across an amazing blog post explaining how to add them with some sample code. After following the tutorial and modifying the HTML and CSS a little bit to better fit with how I want the buttons to be displayed, I am pleased to say that you can now click the share buttons to quickly post a link to any of my content on many different platforms and even email it. I’ve got support for Facebook, Twitter, Reddit, Google Plus, Tumblr and LinkedIn enabled. One feature of this solution that I really like is that the buttons have icons on them from the Font Awesome project. These are all vector social logos that should render quite nicely on any modern browser. Feel free to try it out below if you’d like. It will prompt you to edit the post before sharing it.</summary></entry><entry><title type="html">Blocking Annoying Web Advertisements at the DNS Level with Pi-Hole</title><link href="https://dylanmtaylor.com/pages/blog/2018/05/03/blocking_annoying_web_advertisements_at_the_dns_level_with_pi-hole/" rel="alternate" type="text/html" title="Blocking Annoying Web Advertisements at the DNS Level with Pi-Hole" /><published>2018-05-03T22:39:19+00:00</published><updated>2018-05-03T22:39:19+00:00</updated><id>https://dylanmtaylor.com/pages/blog/2018/05/03/blocking_annoying_web_advertisements_at_the_dns_level_with_pi-hole</id><content type="html" xml:base="https://dylanmtaylor.com/pages/blog/2018/05/03/blocking_annoying_web_advertisements_at_the_dns_level_with_pi-hole/">&lt;p&gt;&lt;a href=&quot;https://pi-hole.net/&quot;&gt;&lt;img src=&quot;/images/blog/2018/05/Vortex-R.png&quot; alt=&quot;Pi-Hole&quot; width=&quot;100px&quot; height=&quot;100px&quot; style=&quot;float:left&quot; /&gt;&lt;/a&gt; If you’re like myself and many other users on the internet, you probably hate advertisements. Not only are they annoying with automatically playing videos, covering up web content and being a vector for malware transmission from unscrupulous advertisement platform, but they can drain your battern on your phone and eat up your bandwidth if you are on a fixed data plan.&lt;/p&gt;

&lt;p&gt;There are many reasons to hate advertisements but fortunately users are fighting back. One way is through browser extensions. I personally use a free and open source one called &lt;a href=&quot;https://github.com/gorhill/uBlock&quot;&gt;uBlock Origin&lt;/a&gt; on all of my devices. It is independently operated, lightweight and does not support the acceptable ads program, unlike Adblok Plus, which is partially funded by advertisers paying to get unblocked. See the conflict of interest here?&lt;/p&gt;

&lt;p&gt;While these extensions are fantastic, they’re only a start. Unfortunately, at least in mobile applications like the one from &lt;a href=&quot;https://www.wunderground.com/&quot;&gt;Weather Underground&lt;/a&gt;, which is quite a good app by the way, there is no way to block advertisements without paying a fee to the developer. The mobile version of Chrome also does not support extensions. I’ve lot count of the number of times I’ve seen fake “Your Android has been infected” pages that make noises and vibrate the phone. It’s obnoxious. Until this isn’t the case anymore, browsing the mobile web without an adblocker is a chore.&lt;/p&gt;

&lt;p&gt;So, how can we block ads across all the devices in the network, including phones without installing special software on anything? Simple: by running our own DNS server. DNS, short for Domain Name System, is a technology that handles domain lookups. When you type dylanmtaylor.com into your browser, your browser doesn’t know where to go to make a connection from the name. It contacts a DNS server to translate the humanly readable domain name into an IP address, which your browser then uses to make the connection. If we intentionally override this using our own DNS server, we can point adomain to something else, like the DNS server itself or 127.0.0.1. By doing this, the domain is effectively blocked.&lt;/p&gt;

&lt;p&gt;This is the core concept behind &lt;a href=&quot;https://pi-hole.net/&quot;&gt;Pi-Hole&lt;/a&gt;. With nothing more than a cheap &lt;a href=&quot;https://www.raspberrypi.org/&quot;&gt;Raspberry Pi&lt;/a&gt; single board computer, you can run your own DNS server. The setup is simple and described on the &lt;a href=&quot;https://pi-hole.net/&quot;&gt;Pi-Hole&lt;/a&gt; website. Simply connect your Pi to the network, setup static IP address allocation for the Pi’s MAC address in your router’s admin interfeace, install the software (probably on top of &lt;a href=&quot;https://www.raspberrypi.org/downloads/raspbian/&quot;&gt;Raspbian Lite&lt;/a&gt;), and finally reconfigure the DNS server entries of your router to point to the local Pi-Hole server. By doing this, so long as you are connected to your wireless network, you will be exposed to significantly less advertisements unless you manually configure your devices to use an alternative DNS server such as 1.1.1.1 from CloudFlare. If you enjoy Pi-Hole, I’d like to suggest &lt;a href=&quot;https://pi-hole.net/donate/&quot;&gt;donating a few dollars to them&lt;/a&gt;. The project is maintained entirely by volunteers and could use all the help they can get to fund the development.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Dylan Taylor&quot;, &quot;login&quot;=&gt;&quot;dylanmtaylor&quot;, &quot;email&quot;=&gt;&quot;dylan@dylanmtaylor.com&quot;}</name><email>dylan@dylanmtaylor.com</email></author><summary type="html">If you’re like myself and many other users on the internet, you probably hate advertisements. Not only are they annoying with automatically playing videos, covering up web content and being a vector for malware transmission from unscrupulous advertisement platform, but they can drain your battern on your phone and eat up your bandwidth if you are on a fixed data plan.</summary></entry><entry><title type="html">Celebrate World Password Day with a Password Manager</title><link href="https://dylanmtaylor.com/pages/blog/2018/05/03/celebrate_world_password_day_with_a_password_manager/" rel="alternate" type="text/html" title="Celebrate World Password Day with a Password Manager" /><published>2018-05-03T22:08:11+00:00</published><updated>2018-05-03T22:08:11+00:00</updated><id>https://dylanmtaylor.com/pages/blog/2018/05/03/celebrate_world_password_day_with_a_password_manager</id><content type="html" xml:base="https://dylanmtaylor.com/pages/blog/2018/05/03/celebrate_world_password_day_with_a_password_manager/">&lt;p&gt;Did you know that May 3rd is &lt;a href=&quot;https://www.passwordday.org/&quot;&gt;World Password Day&lt;/a&gt;? One of the things that I suggest to all my friends is that they have a different password for every single login. I personally do this with the help of a tool called a password manager. Password managers can generate a different password for everything you use and remember it for you. I will freely admit that I don’t know 99% of my passwords, and that is a good thing, since not only are they all rather long and complex (usually I will use the longest length the site allows up to 99 characters, with maximum complexity), but they are also all unique. This is important since if you use the same password for everything you are at risk - criminals are constantly breaking into password databases, and if they manager to get one of your passwords, they have them all.&lt;/p&gt;

&lt;p&gt;There are a number of really good password managers out there. I personally use one called &lt;a href=&quot;https://www.lastpass.com/&quot;&gt;LastPass&lt;/a&gt;, and pay for the subscription. The service is worth it to me since I like to have my passwords synced across multiple devices, but you can also use it for free. Unfortunately, LastPass is not an open source tool, and you have to trust that they are protecting your data adequately.&lt;/p&gt;

&lt;p&gt;If you want a free and open source password manager, I can highly recommend &lt;a href=&quot;https://www.keepassx.org/&quot;&gt;KeePassX&lt;/a&gt;. It has the added benefit of not having to worry about syncing your data to the cloud - you control your own password list entirely. A coworker of mine introduced me to this tool, and I have since tried it out. It works really well, and can even automatically type in passwords. The only downside is that you have to worry about syncing your data manually if you wish to do so, and you are responsible for keeping password backups. If you lose these, getting access to your accounts will be quite painful.&lt;/p&gt;

&lt;p&gt;In observance of world password day, I will be reviewing and changing my older passwords in LastPass, and I suggest that everyone else do the same. This improves your overall security, and removes access to your accounts in the case that someone has managed to get your old password.&lt;/p&gt;

&lt;p&gt;While I’m writing about security, it is worth mentioning that in many cases, having a password alone is not sufficient for proper online security. One thing that everyone should be taking advantage of, especially because of the ubiquity of smartphones is two-factor authentication, or 2FA. 2FA is the idea of providing two pieces of data to login to your account - one that is based on something you know, a password, and one that is based on something you have.&lt;/p&gt;

&lt;p&gt;With modern 2FA like &lt;a href=&quot;https://play.google.com/store/apps/details?id=com.google.android.apps.authenticator2&amp;amp;hl=en_US&quot;&gt;Google Authenticator&lt;/a&gt;, a pre-shared secret key is used to mathematically generate a numerical token based on the current time. This token is only valid for 30 seconds or so. When you enroll your device, the token is only sent to the device once and cannot be retrieved afterwards, so it cannot be duplicated. Because of this, you need your phone or authenticator application, such as &lt;a href=&quot;https://authy.com/&quot;&gt;Authy&lt;/a&gt; in order to get the token needed to log in. I use two-factor authentication on pretty much everything that supports it, and this provides me with an extra layer of security, as even if you can compromise the password, a malicious actor would need to have physical access to the device containing the token in order to login.&lt;/p&gt;

&lt;p&gt;Anyways, happy password day, and stay safe out there!&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Dylan Taylor&quot;, &quot;login&quot;=&gt;&quot;dylanmtaylor&quot;, &quot;email&quot;=&gt;&quot;dylan@dylanmtaylor.com&quot;}</name><email>dylan@dylanmtaylor.com</email></author><summary type="html">Did you know that May 3rd is World Password Day? One of the things that I suggest to all my friends is that they have a different password for every single login. I personally do this with the help of a tool called a password manager. Password managers can generate a different password for everything you use and remember it for you. I will freely admit that I don’t know 99% of my passwords, and that is a good thing, since not only are they all rather long and complex (usually I will use the longest length the site allows up to 99 characters, with maximum complexity), but they are also all unique. This is important since if you use the same password for everything you are at risk - criminals are constantly breaking into password databases, and if they manager to get one of your passwords, they have them all.</summary></entry><entry><title type="html">Checking What IP Addresses Connected to Your Nginx Server</title><link href="https://dylanmtaylor.com/pages/blog/2018/05/01/checking_what_ip_addresses_connected_to_your_nginx_server/" rel="alternate" type="text/html" title="Checking What IP Addresses Connected to Your Nginx Server" /><published>2018-05-01T03:12:25+00:00</published><updated>2018-05-01T03:12:25+00:00</updated><id>https://dylanmtaylor.com/pages/blog/2018/05/01/checking_what_ip_addresses_connected_to_your_nginx_server</id><content type="html" xml:base="https://dylanmtaylor.com/pages/blog/2018/05/01/checking_what_ip_addresses_connected_to_your_nginx_server/">&lt;p&gt;My server is behind a &lt;a href=&quot;https://www.cloudflare.com&quot;&gt;CloudFlare&lt;/a&gt; caching proxy, so it usually doesn’t get hit with requests directly, but I was noticing thousands of unique IP addresses looking for files such as &lt;code&gt;/xampp/phpmyadmin/index.php&lt;/code&gt; and &lt;code&gt;/db/index.php&lt;/code&gt;. These are probably hackers using automated vulnerability scanners against anything that has an open HTTP connection on the internet. I got curious where these requests were coming from, so I decided to dig through the logs.&lt;/p&gt;

&lt;p&gt;The first thing I did was connect to my server using the hostname I have in my &lt;code&gt;/etc/hosts&lt;/code&gt; file for convenience through the GNOME file browser. I entered &lt;code&gt;sftp://root@dylanmtaylor/var/log/nginx&lt;/code&gt; as the connection string. I then copied all the access log files to my local machine into a temporary NGINX folder.&lt;/p&gt;

&lt;p&gt;From there, I ran this commmand:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;for file in $(ls *.gz); do gzip -d $file; done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This decompressed all of the gzipped log files by taking the output of &lt;code&gt;ls *.gz&lt;/code&gt; and iterating through the files to run &lt;code&gt;gzip -d&lt;/code&gt; on each one. For usage information on &lt;code&gt;gzip&lt;/code&gt;, you can run &lt;code&gt;gzip -h&lt;/code&gt;
In order to combine the log files into one for easy parsing, we can use &lt;code&gt;cat&lt;/code&gt;, which is short for concatenate. &lt;code&gt;cat access*.log*&lt;/code&gt; works effectively for this.&lt;/p&gt;

&lt;p&gt;Using UNIX input redirection, we take the output of the &lt;code&gt;cat access*.log*&lt;/code&gt; command and redirect it to the standard input of the awk process. One really useful trick of awk is to print a specific argument to it, like &lt;code&gt;'{print $1}'&lt;/code&gt; which will give you the first column of output. You can change the &lt;code&gt;$1&lt;/code&gt; to the column number of the input desired for automated parsing. I then piped that to &lt;code&gt;uniq -c&lt;/code&gt;. uniq is a utility that will find unique entries. The &lt;code&gt;-c&lt;/code&gt; argument simply shows the count – how many times each occurence showed up. Finally, because these are in no particular order, I yet again piped the output to &lt;code&gt;sort -nr&lt;/code&gt;. The &lt;code&gt;-n&lt;/code&gt; flag simply tells sort to compare based on numerical value. By default, these values are ascending (112 vs 64 would have 64 first). Because I want them in descending order, I added the &lt;code&gt;-r&lt;/code&gt; flag, which puts the ones that show up the most at the top of the list. Finally, I use &lt;code&gt;tee&lt;/code&gt; to store the results as &lt;code&gt;ip-addresses&lt;/code&gt;. The whole command looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;awk '{print $1}' &amp;lt;(cat access*.log*) | sort | uniq | sort -nr | tee ip-addresses
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I then tried to use &lt;a href=&quot;https://www.infobyip.com/ipbulklookup.php&quot;&gt;a bulk IP address lookup tool&lt;/a&gt; to check where these requests were coming from. Unfortunately, they have a limit of 100 IP addresses and they want the list to be space/new line delimited. Fortunately, there is a way to find the worst offenders. The list is already sorted descending, so I can take advantage of the &lt;code&gt;head&lt;/code&gt; tool with the &lt;code&gt;-n 100&lt;/code&gt; argument, which just takes the first 100 lines of the file or standard input and redirects it to standard output.&lt;/p&gt;

&lt;p&gt;Because I saved my output as &lt;code&gt;ip-addresses&lt;/code&gt;, I was able to simply cat this file to the necessary utilities with &lt;code&gt;cat ip-addresses | head -n 100 | awk '{print $2}'&lt;/code&gt;, but UNIX pipes are pretty flexible. The whole thing can be done in one line like so:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;awk '{print $1}' &amp;lt;(cat access*.log*) | sort | uniq | sort -nr | head -n 100 | awk '{print $2}'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Naturally, I found my own IP address and several IP addresses belonging to CloudFlare, which is expected, but I also found hits from assorted IP addresses all over the world, excluding the CloudFlare mirrors (which are in several countries), from the United States, Canada, Indonesia, China, Hong Kong, Seychelles and Russia. From this data, it is hard to easily tell which ones are malicious, and which stumbled upon the IP address, but it is fascinating to see how geographically distributed the requests to a site as small as mine are.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Dylan Taylor&quot;, &quot;login&quot;=&gt;&quot;dylanmtaylor&quot;, &quot;email&quot;=&gt;&quot;dylan@dylanmtaylor.com&quot;}</name><email>dylan@dylanmtaylor.com</email></author><summary type="html">My server is behind a CloudFlare caching proxy, so it usually doesn’t get hit with requests directly, but I was noticing thousands of unique IP addresses looking for files such as /xampp/phpmyadmin/index.php and /db/index.php. These are probably hackers using automated vulnerability scanners against anything that has an open HTTP connection on the internet. I got curious where these requests were coming from, so I decided to dig through the logs.</summary></entry></feed>